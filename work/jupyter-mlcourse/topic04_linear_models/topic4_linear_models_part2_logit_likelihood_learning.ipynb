{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению\n",
    "<center>Автор материала: Юрий Кашницкий\n",
    "    \n",
    "Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Тема 4. Линейные модели классификации и регрессии\n",
    "## <center>Часть 2. Логистическая регрессия и метод максимального правдоподобия "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея линейного классификатора заключается в том, что признаковое пространство может быть разделено гиперплоскостью на две полуплоскости, в каждой из которых прогнозируется одно из двух значений целевого класса. \n",
    "Если это можно сделать без ошибок, то обучающая выборка называется *линейно разделимой*.\n",
    "\n",
    "<img src=\"../../img/logit.png\">\n",
    "\n",
    "Мы уже знакомы с линейной регрессией и методом наименьших квадратов. Рассмотрим задачу бинарной классификации, причем метки целевого класса обозначим \"+1\" (положительные примеры) и \"-1\" (отрицательные примеры).\n",
    "Один из самых простых линейных классификаторов получается на основе регрессии вот таким образом:\n",
    "\n",
    "$$\\Large a(\\textbf{x}) = \\text{sign}(\\textbf{w}^{\\text{T}}\\textbf x),$$\n",
    "\n",
    "где\n",
    " - $\\textbf{x}$ – вектор признаков примера (вместе с единицей);\n",
    " - $\\textbf{w}$ – веса в линейной модели (вместе со смещением $w_0$);\n",
    " - $\\text{sign}(\\bullet)$ – функция \"сигнум\", возвращающая знак своего аргумента;\n",
    " - $a(\\textbf{x})$ – ответ классификатора на примере $\\textbf{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия как линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая регрессия является частным случаем линейного классификатора, но она обладает хорошим \"умением\" – прогнозировать вероятность $p_+$ отнесения примера $\\textbf{x}_\\text{i}$ к классу \"+\":\n",
    "$$\\Large p_+ = \\text P\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) $$\n",
    "\n",
    "Прогнозирование не просто ответа (\"+1\" или \"-1\"), а именно *вероятности* отнесения к классу \"+1\" во многих задачах является очень важным бизнес-требованием. Например, в задаче кредитного скоринга, где традиционно применяется логистическая регрессия, часто прогнозируют вероятность невозврата кредита ($p_+$). Клиентов, обратившихся за кредитом, сортируют по этой предсказанной вероятности (по убыванию), и получается скоркарта — по сути, рейтинг клиентов от плохих к хорошим. Ниже приведен игрушечный пример такой скоркарты. \n",
    "    <img src='../../img/toy_scorecard.png' width=60%>\n",
    "\n",
    "Банк выбирает для себя порог $p_*$ предсказанной вероятности невозврата кредита (на картинке – $0.15$) и начиная с этого значения уже не выдает кредит. Более того, можно умножить предсказнную вероятность на выданную сумму и получить матожидание потерь с клиента, что тоже будет хорошей бизнес-метрикой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы хотим прогнозировать вероятность $p_+ \\in [0,1]$, а пока умеем строить линейный прогноз с помощью МНК: $b(\\textbf{x}) = \\textbf{w}^\\text{T} \\textbf{x} \\in \\mathbb{R}$. Каким образом преобразовать полученное значение в вероятность, пределы которой – [0, 1]? Очевидно, для этого нужна некоторая функция $f: \\mathbb{R} \\rightarrow [0,1].$ В модели логистической регрессии для этого берется конкретная функция: $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$. И сейчас разберемся, каковы для этого предпосылки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return 1. / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(-10, 10, 1000)\n",
    "plt.plot(xx, [sigma(x) for x in xx]);\n",
    "plt.xlabel('z');\n",
    "plt.ylabel('sigmoid(z)')\n",
    "plt.title('Sigmoid function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим $P(X)$ вероятностью происходящего события $X$. Тогда отношение вероятностей $OR(X)$ определяется из $\\frac{P(X)}{1-P(X)}$, а это — отношение вероятностей того, произойдет ли событие или не произойдет. Очевидно, что вероятность и отношение шансов содержат одинаковую информацию. Но в то время как $P(X)$ находится в пределах от 0 до 1, $OR(X)$ находится в пределах от 0 до $\\infty$.\n",
    "\n",
    "Если вычислить логарифм $OR(X)$ (то есть называется логарифм шансов, или логарифм отношения вероятностей), то легко заметить, что $\\log{OR(X)} \\in \\mathbb{R}$. Его то мы и будем прогнозировать с помощью МНК.\n",
    "\n",
    "Посмотрим, как логистическая регрессия будет делать прогноз $p_+ = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)$ (пока считаем, что веса $\\textbf{w}$ мы как-то получили (т.е. обучили модель), далее разберемся, как именно). \n",
    "\n",
    "**Шаг 1.** Вычислить значение $w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \\textbf{w}^\\text{T}\\textbf{x}$. (уравнение $\\textbf{w}^\\text{T}\\textbf{x} = 0$ задает гиперплоскость, разделяющую примеры на 2 класса);\n",
    "\n",
    "\n",
    "**Шаг 2.** Вычислить логарифм отношения шансов: $ \\log(OR_{+}) =  \\textbf{w}^\\text{T}\\textbf{x}$.\n",
    "\n",
    "**Шаг 3.** Имея прогноз шансов на отнесение к классу \"+\" – $OR_{+}$, вычислить $p_{+}$ с помощью простой зависимости:\n",
    "\n",
    "$$\\Large p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\textbf{w}^\\text{T}\\textbf{x}}}{1 + \\exp^{\\textbf{w}^\\text{T}\\textbf{x}}} =  \\frac{1}{1 + \\exp^{-\\textbf{w}^\\text{T}\\textbf{x}}} = \\sigma(\\textbf{w}^\\text{T}\\textbf{x})$$\n",
    "\n",
    "\n",
    "В правой части мы получили как раз сигмоид-функцию.\n",
    "\n",
    "Итак, логистическая регрессия прогнозирует вероятность отнесения примера к классу \"+\" (при условии, что мы знаем его признаки и веса модели) как сигмоид-преобразование линейной комбинации вектора весов модели и вектора признаков примера:\n",
    "\n",
    "$$\\Large p_+(x_i) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}). $$\n",
    "\n",
    "Следующий вопрос: как модель обучается. Тут мы опять обращаемся к принципу максимального правдоподобия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Принцип максимального правдоподобия и логистическая регрессия\n",
    "Теперь посмотрим, как из принципа максимального правдоподобия получается оптимизационная задача, которую решает логистическая регрессия, а именно, – минимизация *логистической* функции потерь. \n",
    "Только что мы увидели, что логистическая регрессия моделирует вероятность отнесения примера к классу \"+\" как \n",
    "\n",
    "$$\\Large p_+(\\textbf{x}_\\text{i}) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Тогда для класса \"-\" аналогичная вероятность:\n",
    "$$\\Large p_-(\\textbf{x}_\\text{i})  = \\text{P}\\left(y_i = -1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)  = 1 - \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) = \\sigma(-\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) $$\n",
    "\n",
    "Оба этих выражения можно ловко объединить в одно (следите за моими руками – не обманывают ли вас):\n",
    "\n",
    "$$\\Large \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}$ называется *отступом* (*margin*) классификации на объекте $\\textbf{x}_\\text{i}$ (не путать с зазором (тоже margin), про который чаще всего говорят в контексте SVM). Если он неотрицателен, модель не ошибается на объекте $\\textbf{x}_\\text{i}$, если же отрицателен – значит, класс для $\\textbf{x}_\\text{i}$  спрогнозирован неправильно. \n",
    "Заметим, что отступ определен для объектов именно обучающей выборки, для которых известны реальные метки целевого класса $y_i$.\n",
    "\n",
    "Чтобы понять, почему это мы сделали такие выводы, обратимся к геометрической интерпретации линейного классификатора. Подробно про это можно почитать в материалах Евгения Соколова – [тут](https://github.com/esokolov/ml-course-msu/blob/master/ML16/lecture-notes/Sem09_linear.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рекомендую решить почти классическую задачу из начального курса линейной алгебры: найти расстояние от точки с радиус-вектором $\\textbf{x}_A$ до плоскости, которая задается уравнением $\\textbf{w}^\\text{T}\\textbf{x} = 0.$\n",
    "\n",
    "\n",
    "Ответ: \n",
    "$\\Large \\rho(\\textbf{x}_A, \\textbf{w}^\\text{T}\\textbf{x} = 0) = \\frac{\\textbf{w}^\\text{T}\\textbf{x}_A}{||\\textbf{w}||}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/simple_linal_task.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда получим (или посмотрим) ответ, то поймем, что чем больше по модулю выражение $\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$, тем дальше точка $\\textbf{x}_\\text{i}$ находится от плоскости $\\textbf{w}^{\\text{T}}\\textbf{x} = 0.$\n",
    "\n",
    "Значит, выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}$ – это своего рода \"уверенность\" модели в классификации объекта $\\textbf{x}_\\text{i}$: \n",
    "\n",
    "- если отступ большой (по модулю) и положительный, это значит, что метка класса поставлена правильно, а объект находится далеко от разделяющей гиперплоскости (такой объект классифицируется уверенно). На рисунке – $x_3$.\n",
    "- если отступ большой (по модулю) и отрицательный, значит метка класса поставлена неправильно, а объект находится далеко от разделюящей гиперплоскости (скорее всего такой объект – аномалия, например, его метка в обучающей выборке поставлена неправильно). На рисунке – $x_1$.\n",
    "- если отступ малый (по модулю), то объект находится близко к разделюящей гиперплоскости, а  знак отступа определяет, правильно ли объект классифицирован.  На рисунке – $x_2$ и $x_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь распишем правдоподобие выборки, а именно, вероятность наблюдать данный вектор $\\textbf{y}$ у выборки $\\textbf X$. Делаем сильное предположение: объекты приходят независимо, из одного распределения (*i.i.d.*). Тогда\n",
    "\n",
    "$$\\Large \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\prod_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right),$$\n",
    "\n",
    "где $\\ell$ – длина выборки $\\textbf X$ (число строк).\n",
    "\n",
    "Как водится, возьмем логарифм данного выражения (сумму оптимизировать намного проще, чем произведение):\n",
    "\n",
    "$$\\Large  \\log \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\log \\sum_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i})   = $$\n",
    "\n",
    "$$\\Large  = \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}) = \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}} = - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть в данном случае принцип максимизации правдоподобия приводит к минимизации выражения \n",
    "\n",
    "$$\\Large \\mathcal{L_{log}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}).$$\n",
    "\n",
    "Это *логистическая* функция потерь, просуммированная по всем объектам обучающей выборки.\n",
    "\n",
    "Посмотрим на новую фунцию как на функцию от отступа: $L(M) = \\log (1 + \\exp^{-M})$. Нарисуем ее график, а также график 1/0 функциий потерь (*zero-one loss*), которая просто штрафует модель на 1 за ошибку на каждом объекте (отступ отрицательный): $L_{1/0}(M) = [M < 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../img/logloss_margin.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинка отражает общую идею, что в задаче классификации, не умея напрямую минимизировать число ошибок (по крайней мере, градиентными методами это не сделать – производная 1/0 функциий потерь в нуле обращается в бесконечность), мы минимизируем некоторую ее верхнюю оценку. В данном случае это логистическая функция потерь (где логарифм двоичный, но это не принципиально), и справедливо \n",
    "\n",
    "$$\\Large \\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} [M(\\textbf{x}_\\text{i}) < 0] \\leq \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}), $$\n",
    "\n",
    "где $\\mathcal{L_{\\text{1/0}}} (\\textbf X, \\textbf{y}, \\textbf{w})$ – попросту число ошибок логистической регрессии с весами $\\textbf{w}$ на выборке $(\\textbf X, \\textbf{y})$.\n",
    "\n",
    "То есть уменьшая верхнюю оценку $\\mathcal{L_{\\log}}$ на число ошибок классификации, мы таким образом надеемся уменьшить и само число ошибок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-регуляризация логистической функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L2-регуляризация$ логистической регрессии устроена почти так же, как и в случае с гребневой (Ridge регрессией). Вместо функционала $\\mathcal{L_{\\log}} (X, \\textbf{y}, \\textbf{w})$ минимизируется следующий:\n",
    "\n",
    "$$\\Large J(\\textbf X, \\textbf{y}, \\textbf{w}) = \\mathcal{L_{\\log}} (\\textbf X, \\textbf{y}, \\textbf{w}) + \\lambda |\\textbf{w}|^2$$\n",
    "\n",
    "В случае логистической регрессии принято введение обратного коэффициента регуляризации $C = \\frac{1}{\\lambda}$. И тогда решением задачи будет\n",
    "\n",
    "$$\\Large \\widehat{\\textbf{w}}  = \\arg \\min_{\\textbf{w}} J(\\textbf X, \\textbf{y}, \\textbf{w}) =  \\arg \\min_{\\textbf{w}}\\ (C\\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})+ |\\textbf{w}|^2)$$ \n",
    "\n",
    "Далее рассмотрим пример, позволяющий интуитивно понять один из смыслов регуляризации. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
